{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3022d0fe-a7ae-4f1d-a882-3cda85083179",
   "metadata": {},
   "source": [
    "# Importing Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae87a5f2-a156-488f-b227-57555b886f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys                                                                       ## System and Utility Packages\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import numpy as np                                                               ## Load and pre-process the data\n",
    "import pandas as pd\n",
    "\n",
    "import torch                                                                     ## PyTorch library for tensor operations\n",
    "import torch.nn as nn                                                            ## Builds classification head on top of BERT\n",
    "from torch.optim import AdamW                                                    ## Optimizer for training neural networks\n",
    "from torch.utils.data import DataLoader                                          ## Utilities for handling datasets and batching\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel      ## Tokenizer to create tokens and model for NLP tasks\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight                      ## Machine learning utilities to compute weights\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score      ## Evaluate model performance\n",
    "\n",
    "from tqdm import tqdm                                                            ## Progress tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3776bad6-3153-480e-b74f-98db58170f1b",
   "metadata": {},
   "source": [
    "### Loading the data from kaggle and instagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7966a6d7-8532-4f0f-a646-c18d8a00cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_1 = \"/users/sgamahaw/data/HateSpeechDataset.csv\"\n",
    "file_path_2 = \"/users/sgamahaw/data/ig_data.csv\"\n",
    "\n",
    "kaggle_df = pd.read_csv(file_path_1)\n",
    "ig_df = pd.read_csv(file_path_2)\n",
    "\n",
    "kaggle_df = kaggle_df.rename(columns={\n",
    "    'Content': 'text',\n",
    "    'Label': 'hate_speech'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413f146a-903c-462e-afc2-1d07af40d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Cleaning\n",
    "kaggle_df = kaggle_df[['text', 'hate_speech']]\n",
    "\n",
    "# Keep only rows where Label is 0 or 1\n",
    "kaggle_df['hate_speech'] = pd.to_numeric(kaggle_df['hate_speech'], errors='coerce')\n",
    "kaggle_df = kaggle_df.dropna(subset=['hate_speech'])\n",
    "kaggle_df['hate_speech'] = kaggle_df['hate_speech'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fb78ee-bf1b-4634-bfab-2870543d1e0c",
   "metadata": {},
   "source": [
    "### Compute Class Weights on Training data - Kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c3a3ca-48d2-4c71-82e3-524edff76051",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(kaggle_df['hate_speech'])\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=kaggle_df['hate_speech'].values)\n",
    "\n",
    "ig_df['hate_speech'] = pd.to_numeric(ig_df['hate_speech'], errors='coerce')\n",
    "ig_df = ig_df.dropna(subset=['hate_speech'])\n",
    "ig_df['hate_speech'] = ig_df['hate_speech'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26dcf27-1f5f-4b37-b599-23c1ad4d7dfa",
   "metadata": {},
   "source": [
    "# Tokenizer and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b41a46-8f06-4bf8-b69f-ae55f9577847",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Preprocess(Dataset):\n",
    "\n",
    "    '''This class stores the tokenizer and the dataframe and extracts the text column.\n",
    "        Converts the hatespeech column into one-hot encoded format and stores max token length per sentence.\n",
    "        Returns everything in tensor format so it can be used in training loops or DataLoaders.'''\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = torch.tensor(self.data[\"hate_speech\"].values, dtype=torch.float)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    '''Returns number of samples in the dataset which is required by DataLoader'''\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    '''Cleans up text by removing extra whitespace and tokenizes the text using BERTâ€™s tokenizer.\n",
    "    Adds special tokens, truncates to max length and returns input_ids, attention_mask, and token_type_ids.\n",
    "    Create tensors and return as a dictionary containing ids, mask, token_type_ids and targets'''\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text.iloc[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',         # Proper padding\n",
    "            truncation=True,              # Truncate if too long\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"targets\": self.targets[index]\n",
    "        }\n",
    "\n",
    "training_set = Dataset_Preprocess(kaggle_df, TOKENIZER, MAX_LEN)\n",
    "testing_set = Dataset_Preprocess(ig_df, TOKENIZER, MAX_LEN)\n",
    "\n",
    "# Creating dictionaries containing arguments for the DataLoader.\n",
    "# They're used to control how batches are loaded during training, validation, and testing\n",
    "\n",
    "train_params = {\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 0\n",
    "}\n",
    "\n",
    "test_params = {\n",
    "    \"batch_size\": 1,\n",
    "    \"shuffle\": False,\n",
    "    \"num_workers\": 0\n",
    "}\n",
    "\n",
    "# DataLoader Creation - Each DataLoader will now efficiently load batches of data and\n",
    "# yield the preprocessed tensors (ids, mask, targets, etc.) from your Dataset_Preprocess class\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9178bd7e-efd2-434c-a377-81289221cb25",
   "metadata": {},
   "source": [
    "## Parameters for Muril/BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a59d395-2826-4392-b704-f0e8c7d0a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/muril-base-cased\"\n",
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 128\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-5\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME, truncation=True)   \n",
    "\n",
    "training_set = Dataset_Preprocess(kaggle_df, TOKENIZER, MAX_LEN)\n",
    "testing_set = Dataset_Preprocess(ig_df, TOKENIZER, MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c63aa1d-6ecd-4df1-84db-c3a37310d7f5",
   "metadata": {},
   "source": [
    "## Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d894cca-1c7c-4df2-a6fb-300f95f0d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Muril(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(CNN_Muril, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "        # CNN layer: Conv1D over token embeddings\n",
    "        self.conv1 = nn.Conv1d(in_channels=768, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.AdaptiveMaxPool1d(1)  # Global max pooling\n",
    "\n",
    "        # Classifier\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(256, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask, \n",
    "                            token_type_ids=token_type_ids)\n",
    "\n",
    "        # Convert to (batch_size, 768, seq_len) for Conv1d\n",
    "        x = outputs.last_hidden_state.permute(0, 2, 1)\n",
    "\n",
    "        # Apply CNN + ReLU + Max Pool\n",
    "        x = self.conv1(x)  # Shape: (batch_size, 256, seq_len)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)  # Shape: (batch_size, 256, 1)\n",
    "        x = x.squeeze(2)     # Shape: (batch_size, 256)\n",
    "\n",
    "        # Classification head\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e59a6-dcbf-42e0-8e34-ca282925af9a",
   "metadata": {},
   "source": [
    "## Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b323f323-b578-4daa-a3ea-0267f474ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device being used:\", device)\n",
    "\n",
    "num_classes = kaggle_df[\"hate_speech\"].nunique()\n",
    "model = CNN_Muril(n_classes = num_classes)\n",
    "model.to(device)\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "class_weights = class_weights.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ec6d63-6126-44de-bb69-1ea83c290342",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034871b9-e41a-4aed-bf4b-4bb3eff5b166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss(weight=class_weights)(outputs, targets.long())\n",
    "\n",
    "optimizer = AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for _, data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data[\"ids\"].to(device, dtype=torch.long)\n",
    "        mask = data[\"mask\"].to(device, dtype=torch.long)\n",
    "        token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "        targets = data[\"targets\"].to(device, dtype=torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(training_loader)\n",
    "    print(f\"Epoch {epoch} average loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b888633-1037-4475-b796-a1a0463221b4",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ba7a2-0b08-41bc-bc85-7646f60f0a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, loader):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(loader, 0)):\n",
    "            ids = data[\"ids\"].to(device, dtype=torch.long)\n",
    "            mask = data[\"mask\"].to(device, dtype=torch.long)\n",
    "            token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "            targets = data[\"targets\"].to(device, dtype=torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cfdaa4-347e-40f9-a308-046b22a04bfd",
   "metadata": {},
   "source": [
    "## Prepares Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff92347-8a19-4084-9314-ff8a5747425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Training Timer Start ---\n",
    "    train_start_time = time.time()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch)\n",
    "\n",
    "    train_end_time = time.time()\n",
    "    train_time = train_end_time - train_start_time\n",
    "    print(f\"\\nTraining Time: {train_time:.2f} seconds ({train_time/60:.2f} minutes)\\n\")\n",
    "\n",
    "\n",
    "    # --- Evaluation Timer Start ---\n",
    "    test_start_time = time.time()\n",
    "\n",
    "    outputs, targets = validation(model, testing_loader)\n",
    "\n",
    "    test_end_time = time.time()\n",
    "    test_time = test_end_time - test_start_time\n",
    "    print(f\"\\nEvaluation Time: {test_time:.2f} seconds ({test_time/60:.2f} minutes)\\n\")\n",
    "\n",
    "    # Process predictions and print report\n",
    "    final_outputs = np.argmax(outputs, axis=1)\n",
    "    targets = np.array(targets).astype(int) \n",
    "\n",
    "\n",
    "    print(f\"Got {sum(final_outputs == targets)} / {len(final_outputs)} correct\")\n",
    "    print(classification_report(targets, final_outputs))\n",
    "\n",
    "    # Save classification report to CSV\n",
    "    report_dict = classification_report(targets, final_outputs, output_dict=True)\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "    report_df.to_csv(\"classification_report.csv\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
